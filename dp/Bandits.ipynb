{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Álvaro López\n",
    "\n",
    "### El problema de las tragaperras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Consideremos en 10-armed-testbed descrito en el libro de Sutton y Barto: 2000 máquinas tragaperras de 10 palancas cada una, todas ellas con retornos aleatorios que siguen una distribución normal. Cada palanca tiene una media generada aleatoriamente de una distribución normal estándar y varianza 1.\n",
    "Consideremos el fichero ten_armed_testbed_2_2.py que genera la figura 2.2 del libro de Sutton y Barto.\n",
    "Modificar el fichero para que el generador de números aleatorios de numpy se inicie siempre a un valor dado (por defecto, 0).**\n",
    "\n",
    "**La función figure_2_2 crea tres objetos de tipo Bandit, uno para cada valor de epsilon. Posteriormente, la función simulate reinicia cada bandido para cada una de las simulaciones, generando nuevos valores para las palancas.**\n",
    "**Supongamos que realizamos la simulación con 500 bandidos, y 5000 partidas en cada uno. Calcular y mostrar el valor óptimo para la mejor palanca de cada uno de los 3 bandidos, promediado para todas las simulaciones.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: matplotlib.pyplot as already been imported, this call will have no effect.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "class Bandit:\n",
    "    def __init__(self, k_arm=10, epsilon=0., initial=0., step_size=0.1, sample_averages=True,\n",
    "                 gradient=False, true_reward=0., sigma=1):\n",
    "        self.k = k_arm\n",
    "        self.step_size = step_size\n",
    "        self.sample_averages = sample_averages\n",
    "        self.indices = np.arange(self.k)\n",
    "        self.time = 0\n",
    "        self.true_reward = true_reward\n",
    "        self.epsilon = epsilon\n",
    "        self.initial = initial\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def reset(self):\n",
    "        # real reward for each action\n",
    "        self.q_true = np.random.randn(self.k) + self.true_reward\n",
    "\n",
    "        # estimation for each action\n",
    "        self.q_estimation = np.zeros(self.k) + self.initial\n",
    "\n",
    "        # # of chosen times for each action\n",
    "        self.action_count = np.zeros(self.k)\n",
    "\n",
    "        self.best_action = np.argmax(self.q_true)\n",
    "        \n",
    "    # get an action for this bandit\n",
    "    def act(self):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.indices)\n",
    "\n",
    "        return np.argmax(self.q_estimation)\n",
    "    \n",
    "    # take an action, update estimation for this action\n",
    "    def step(self, action):\n",
    "        \n",
    "        # generate the reward under N(real reward, 1)\n",
    "        reward = self.sigma * np.random.randn() + self.q_true[action]\n",
    "        self.time += 1\n",
    "        self.action_count[action] += 1\n",
    "\n",
    "        if self.sample_averages:\n",
    "            # update estimation using sample averages\n",
    "             self.q_estimation[action] += 1.0 / self.action_count[action] * (reward - self.q_estimation[action])            \n",
    "        else:\n",
    "            # update estimation with constant step size\n",
    "            self.q_estimation[action] += self.step_size * (reward - self.q_estimation[action])  \n",
    "            \n",
    "        return reward      \n",
    "\n",
    "def simulate(runs, time, bandits):\n",
    "    best_action_counts = np.zeros((len(bandits), runs, time))\n",
    "    rewards = np.zeros(best_action_counts.shape)\n",
    "    estimaciones = np.zeros(3)\n",
    "    for i, bandit in enumerate(bandits):\n",
    "        cont = 0\n",
    "        for r in range(runs):\n",
    "            bandit.reset()\n",
    "            cont += np.max(bandit.q_true)\n",
    "            for t in range(time):\n",
    "                action = bandit.act()\n",
    "                reward = bandit.step(action)\n",
    "                rewards[i, r, t] = reward\n",
    "                if action == bandit.best_action:\n",
    "                    best_action_counts[i, r, t] = 1\n",
    "            estimaciones[i] += np.max(bandit.q_true)\n",
    "        #print([n/runs for n in estimaciones])\n",
    "        print(\"Epsilon= %s;Promedio mejor=%s\" % (str(bandit.epsilon), str(cont/runs)))\n",
    "                    \n",
    "    best_action_counts = best_action_counts.mean(axis=1)\n",
    "    rewards = rewards.mean(axis=1)\n",
    "    return best_action_counts, rewards\n",
    "    \n",
    "def figure_2_2(runs=2000, time=1000, sigma=1, valor_inicial = 1, output = \"figure_2_2.png\", title = \"\"):\n",
    "    np.random.seed(0)\n",
    "    epsilons = [0, 0.1, 0.01] \n",
    "    bandits = [Bandit(epsilon=eps, sample_averages=True, sigma=sigma, initial = valor_inicial) for eps in epsilons]\n",
    "    best_action_counts, rewards = simulate(runs, time, bandits)\n",
    "\n",
    "    \n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.title(title)\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for eps, rewards in zip(epsilons, rewards):\n",
    "        plt.plot(rewards, label='epsilon = %.02f' % (eps))\n",
    "    plt.xlabel('steps')\n",
    "    plt.ylabel('average reward')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    for eps, counts in zip(epsilons, best_action_counts):\n",
    "        plt.plot(counts, label='epsilon = %.02f' % (eps))\n",
    "    plt.xlabel('steps')\n",
    "    plt.ylabel('% optimal action')\n",
    "    plt.legend()\n",
    "    plt.ioff()\n",
    "    plt.savefig(output)\n",
    "\n",
    "    #plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solucion:\n",
    "* Tragaperras de 10 palancas cada una, todas ellas con retornos aleatorios que siguen una distribución normal\n",
    "* Cada palanca tiene una media generada aleatoriamente de una distribución normal estándar y varianza 1 (sigma=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon= 0;Promedio mejor=1.5627250745562002\n",
      "Epsilon= 0.1;Promedio mejor=1.5324017139984836\n",
      "Epsilon= 0.01;Promedio mejor=1.4964421909035448\n"
     ]
    }
   ],
   "source": [
    "figure_2_2(runs=500, time=5000, sigma=1, output=\"solu1.png\", title=\"N=500 Tragaperras de 10 palancas cada una, varianza=1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](solu1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Obtener las gráficas (Average Reward /Plays) y (% Optimal action / Plays) a partir del fichero proporcionado. Evaluar únicamente 500 máquinas tragaperras realizando 5000 partidas con cada una. Repetir el experimento con sigma = 2 y con sigma = 0.5**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solución:\n",
    "\n",
    "### Sigma=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon= 0;Promedio mejor=1.5627250745562002\n",
      "Epsilon= 0.1;Promedio mejor=1.5324017139984836\n",
      "Epsilon= 0.01;Promedio mejor=1.4964421909035448\n"
     ]
    }
   ],
   "source": [
    "figure_2_2(runs=500, time=5000, sigma=2, output=\"sol2_sigma2.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](sol2_sigma2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigma = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon= 0;Promedio mejor=1.5627250745562002\n",
      "Epsilon= 0.1;Promedio mejor=1.5324017139984836\n",
      "Epsilon= 0.01;Promedio mejor=1.4964421909035448\n"
     ]
    }
   ],
   "source": [
    "figure_2_2(runs=500, time=5000, sigma=.5, output=\"sol2_sigma0_5.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](sol2_sigma05.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Repetir lo anterior, pero dando 5 como valor inicial de todas las estimaciones. ¿Cómo se ve afectada la eficiencia y convergencia de las distintas estrategias?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon= 0;Promedio mejor=1.5627250745562002\n",
      "Epsilon= 0.1;Promedio mejor=1.5324017139984836\n",
      "Epsilon= 0.01;Promedio mejor=1.4964421909035448\n"
     ]
    }
   ],
   "source": [
    "figure_2_2(runs=500, time=5000, sigma=2, valor_inicial=5, output=\"sol3.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](sol3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon= 0;Promedio mejor=1.5627250745562002\n",
      "Epsilon= 0.1;Promedio mejor=1.5324017139984836\n",
      "Epsilon= 0.01;Promedio mejor=1.4964421909035448\n"
     ]
    }
   ],
   "source": [
    "figure_2_2(runs=500, time=5000, sigma=.5, valor_inicial=5, output=\"sol3_sigma0.5.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](sol3_sigma0.5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Para un sigma=0.5 se han obtenido resultados mas rapidos ya que necesitamos menor número de pasos.\n",
    "* Con un valor inicial alto la convergencia se produce con anterioridad. \n",
    "* Para un sigma = 2 no se producen tales mejoras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.Analizar la clase Bandit y el método step. A medida que se llama al método step, la clase Bandit guarda para cada palanca tanto el número de veces que se ha activado (self.action_count), como el valor estimado para su recompensa (self.q_estimation). Cuando el valor de self.sample_averages es True, las estimaciones de valor de cada palanca se actualizan del siguiente modo: si una palanca se ha activado n veces, la nueva estimación se calculará de la forma:\n",
    "Las estimación inicial de cada acción (palanca) es Q0 = 0 (arbitrariamente), y lo seguirá siendo mientras no se realice (active) al menos una vez. Las estimaciones de las palancas únicamente cambian cuando se activan**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Realizar una función MATLAB que permita mostrar gráficamente la evolución de los valores Q estimados para cada palanca a lo largo del tiempo para una única máquina tragaperras similar a las utilizadas en el 10-armed-testbed**\n",
    "\n",
    "**Los parámetros de la función serán:**\n",
    "• épsilon: porcentaje de exploración\n",
    "• partidas: número de partidas (veces que se accionará alguna palanca)\n",
    "• sigma: desviación estándar del beneficio obtenido de cada palanca.\n",
    "• palancas: número de palancas\n",
    "• semilla: el valor inicial del generador de números aleatorios\n",
    "\n",
    "**Más concretamente, la función realizará lo siguiente:**\n",
    "• mostrará por pantalla los valores medios reales calculados inicialmente para cada palanca.\n",
    "• mostrará en una única gráfica la evolución de los valores de las estimaciones Qt para cada palanca tras simular el número de partidas indicadas. Más concretamente, el eje x mostrará el número de partidas jugadas, y el eje y el valor de la estimación de cada palanca. Se puede tomar como referencia el código del fichero ten_armed_testbed.py.\n",
    "\n",
    "**Realizar una simulación de una máquina con 3 palancas, 200 partidas, desviación estándar 1 y epsilon 0.1. Utilizar para ello un valor de estimación inicial de 0 para todas las palancas. Iniciar el generador de números aleatorios con semilla 11.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class Bandit:\n",
    "    def __init__(self, k_arm=10, epsilon=0.1, initial=0., step_size=0.1, sample_averages=False,\n",
    "                 gradient=False, true_reward=0., sigma=1, modo = 0):\n",
    "        self.modo = modo\n",
    "        self.k = k_arm\n",
    "        self.step_size = step_size\n",
    "        self.sample_averages = sample_averages\n",
    "        self.indices = np.arange(self.k)\n",
    "        self.time = 0\n",
    "        self.true_reward = true_reward\n",
    "        self.epsilon = epsilon\n",
    "        self.initial = initial\n",
    "        self.sigma = sigma\n",
    "        self.palancas = { \"estimaciones\": None, \"reales\": None}\n",
    "        print(self.palancas[\"estimaciones\"])\n",
    "\n",
    "    \n",
    "    def reset(self):\n",
    "        # real reward for each action\n",
    "        self.q_true = np.random.randn(self.k) + self.true_reward\n",
    "\n",
    "        # estimation for each action\n",
    "        self.q_estimation = np.zeros(self.k) + self.initial\n",
    "\n",
    "        # # of chosen times for each action\n",
    "        self.action_count = np.zeros(self.k)\n",
    "\n",
    "        self.best_action = np.argmax(self.q_true)\n",
    "\n",
    "\n",
    "    # get an action for this bandit\n",
    "    def act(self):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.indices)\n",
    "\n",
    "        return np.argmax(self.q_estimation)\n",
    "\n",
    "    # take an action, update estimation for this action\n",
    "    def step(self, action):\n",
    "        reward = self.sigma * np.random.randn() + self.q_true[action]\n",
    "        self.time += 1\n",
    "        self.action_count[action] += 1\n",
    "\n",
    "        # update estimation using sample averages\n",
    "        if self.sample_averages:\n",
    "            # diferentes modos que nos pediran a lo largo de los ejercicios\n",
    "            if self.modo == 0:\n",
    "                self.q_estimation[action]+= 1.0/self.action_count[action] * (reward - self.q_estimation[action])\n",
    "            elif self.modo == 1:\n",
    "                self.q_estimation[action]+= .1*(reward - self.q_estimation[action])\n",
    "            else:\n",
    "                self.q_estimation[action]+= 1.0/self.time * (reward - self.q_estimation[action])            \n",
    "        else:\n",
    "            # update estimation with constant step size\n",
    "            self.q_estimation[action] += self.step_size * (reward - self.q_estimation[action])  \n",
    "            \n",
    "        for p in range(self.k):\n",
    "            if p == action:\n",
    "                self.palancas['estimaciones'][p, self.time - 1] = self.q_estimation[action]\n",
    "            else:\n",
    "                if self.time > 1:\n",
    "                    self.palancas['estimaciones'][p, self.time - 1] = self.palancas['estimaciones'][p, self.time - 2]        \n",
    "            \n",
    "        return reward\n",
    "\n",
    "\n",
    "    def ejercicio4(self, npartidas = 100):\n",
    "        self.reset()\n",
    "        print(\"Palanca, valor medio: %s\" % str(self.q_true))\n",
    "        self.palancas['reales'] = np.zeros((self.k, npartidas))\n",
    "        self.palancas['estimaciones'] = np.zeros((self.k, npartidas))    \n",
    "        for partida in range(npartidas):\n",
    "            palanca = self.act()\n",
    "            self.palancas['reales'][palanca, partida] = self.step(palanca)        \n",
    "        return self.palancas['estimaciones']          \n",
    "    \n",
    "    \n",
    "def simulate(runs, time, bandits):\n",
    "    best_action_counts = np.zeros((len(bandits), runs, time))\n",
    "    rewards = np.zeros(best_action_counts.shape)\n",
    "    for i, bandit in enumerate(bandits):\n",
    "        cont = 0\n",
    "        for r in range(runs):\n",
    "            bandit.reset()\n",
    "            cont += np.max(bandit.q_true)\n",
    "            for t in range(time):\n",
    "                action = bandit.act()\n",
    "                reward = bandit.step(action)\n",
    "                rewards[i, r, t] = reward\n",
    "                if action == bandit.best_action:\n",
    "                    best_action_counts[i, r, t] = 1\n",
    "        print(\"Epsilon= %s;Promedio mejor=%s\" % (str(bandit.epsilon), str(cont/runs)))\n",
    "                    \n",
    "    best_action_counts = best_action_counts.mean(axis=1)\n",
    "    rewards = rewards.mean(axis=1)\n",
    "    return best_action_counts, rewards\n",
    "\n",
    "def simulation(runs=2000, time=1000, modo = 0, output = \"fig4.png\"):\n",
    "    np.random.seed(11)\n",
    "    bandits4 = Bandit(epsilon=0.1,k_arm=3, sample_averages=True, modo = modo)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.subplot(1, 1, 1)\n",
    "    estimaciones = bandits4.ejercicio4(time)\n",
    "    for pal, est in zip(np.arange(3), estimaciones):\n",
    "        plt.plot(est, label='P = {0}'.format(pal))\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Estimation per action')\n",
    "    plt.legend()\n",
    "   \n",
    "    plt.savefig(output)\n",
    "    #plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Palanca, valor medio: [ 1.74945474 -0.286073   -0.48456513]\n"
     ]
    }
   ],
   "source": [
    "simulation(runs=1, time=200, modo = 0, output=\"sol4b.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](sol4b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Repetir el ejercicio 4, pero empleando una tasa de aprendizaje=0.1 constante.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Palanca, valor medio: [ 1.74945474 -0.286073   -0.48456513]\n"
     ]
    }
   ],
   "source": [
    "simulation(runs=1, time=200, modo = 1, output=\"sol5.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](sol5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Repetir el ejercicio 4 pero empleando como tasa de aprendizaje 1/t.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Palanca, valor medio: [ 1.74945474 -0.286073   -0.48456513]\n"
     ]
    }
   ],
   "source": [
    "simulation(runs=1, time=200, modo = 2, output=\"sol6.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](sol6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Reimplementar el problema de los bandidos utilizando dos clases que separen adecuadamente el entorno del agente que actúa sobre el mismo:**\n",
    "•La clase Bandido representará una máquina tragaperras de k palancas. Poseerá internamente un generador de números aleatorios, la semilla inicial, los valores que corresponderán a la media real de cada palanca y su desviación típica. Además poseerá al menos los siguientes métodos:<br/>\n",
    "o reset() - reinicia el generador de números aleatorios con el valor de la semilla, y genera los valores de las medias reales de cada palanca aleatoriamente.<br/>\n",
    "o action_space() - devolverá un entero indicando el número de palancas. Si el bandido tiene N palancas, las acciones posibles serán los valores enteros de 0 hasta N - 1.<br/>\n",
    "o reward = step(self, action) - dada una acción (índice de la palanca que se quiere mover), devolverá la recompensa estocástica tomada de una distribución normal con los parámetros correspondientes.<br/>\n",
    "•La clase Agente representará un agente que está jugando con la máquina. Poseerá internamente los valores necesarios para calcular las estimaciones de las acciones a medida que vaya jugando, su propia semilla y su propio generador de números aleatorios. <br/>\n",
    "•La forma de interacción entre los objetos Bandido y Agente queda a libre elección del programador.\n",
    "Implementar una función que cree un bandido con 3 palancas y un agente. A continuación el agente jugará 200 partidas con el bandido y mostrará la misma gráfica pedida en el apartado 4. Iniciar a 0 tanto el generador de números aleatorios del agente como el del bandido/tragaperras.<br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bandit:\n",
    "    def __init__(self, k_arm=3, step_size=0.1, sample_averages=True,\n",
    "                 gradient=False, true_reward=0., seed=0, valor_inicial=0.):\n",
    "        self.seed = seed\n",
    "        self.rnd = np.random.RandomState(seed=seed)\n",
    "        self.k = k_arm\n",
    "        self.step_size = step_size\n",
    "        self.sample_averages = sample_averages\n",
    "        self.time = 0\n",
    "        self.true_reward = true_reward\n",
    "        self.valor_inicial = valor_inicial\n",
    "        \n",
    "    def action_space():\n",
    "        return self.k\n",
    "\n",
    "    def reset(self):\n",
    "        self.rnd.seed(self.seed)\n",
    "        self.q_true = self.rnd.randn(self.k) + self.true_reward\n",
    "        # real typical deviation of each arm\n",
    "        self.sigmas = np.ones(self.k)\n",
    "        # # of chosen times for each action\n",
    "        self.action_count = np.zeros(self.k)\n",
    "        self.best_action = np.argmax(self.q_true)\n",
    "        \n",
    "        # estimation for each action\n",
    "        self.q_estimation = np.zeros(self.k) + self.valor_inicial\n",
    "\n",
    "    def step(self, action):\n",
    "        # Si vale -1 es porque se va a elegir la politica greedy\n",
    "        if action == -1:\n",
    "            action = np.argmax(self.q_estimation)\n",
    "        # generate the reward under N(real reward, 1), si quiero media 10 y varianza 7 hacemos z=X*7 + 10\n",
    "        reward = self.rnd.randn()*self.sigmas[action] + self.q_true[action]\n",
    "        self.time += 1\n",
    "        self.action_count[action] += 1\n",
    "        if self.sample_averages:\n",
    "            # update estimation using sample averages\n",
    "            self.q_estimation[action] += 1.0 / self.action_count[action] * (reward - self.q_estimation[action])\n",
    "        else:\n",
    "            # update estimation with constant step size\n",
    "            self.q_estimation[action] += self.step_size * (reward - self.q_estimation[action])\n",
    "        return reward\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, k_arm=3, epsilon=0., seed=0):\n",
    "        self.rnd = np.random.RandomState(seed=seed)\n",
    "        self.seed = seed\n",
    "        self.k = k_arm\n",
    "        self.epsilon = epsilon\n",
    "        self.indices = np.arange(self.k)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.rnd.seed(self.seed)\n",
    "    \n",
    "    def play(self):\n",
    "        \"\"\" Politica en cada accion random dependiendo del valor de epsilon\"\"\"\n",
    "        if self.rnd.rand() < self.epsilon:\n",
    "            return self.rnd.choice(self.indices)\n",
    "        return -1\n",
    "\n",
    "\n",
    "def game(iteraciones, agent, bandits):\n",
    "    \"\"\" Realizar una simulacion dado bandits y el agente\"\"\"\n",
    "    agent.reset()\n",
    "    bandits.reset()\n",
    "    \n",
    "    acciones = np.zeros((iteraciones))\n",
    "    estimaciones = np.array([])    \n",
    "    recompensas = np.zeros(acciones.shape)\n",
    "    \n",
    "    \n",
    "   # print(\"Estimaciones\",bandits.q_true)\n",
    "    for nth in range(iteraciones):\n",
    "        action = agent.play()\n",
    "        if action == bandits.best_action:\n",
    "            acciones[nth] = 1\n",
    "\n",
    "        recompensas[nth] = bandit.step(action)\n",
    "        estimaciones = np.append(estimaciones,bandits.q_estimation)\n",
    "\n",
    "    acciones = acciones.mean()\n",
    "    recompensas = recompensas.mean()\n",
    "    print(acciones, recompensas)\n",
    "    return estimaciones\n",
    "\n",
    "def plot(estimaciones, output=\"figure.png\"):\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for palanca, estimacion in zip(np.arange(3), estimaciones):\n",
    "        plt.plot(estimacion, label='P = {0}'.format(palanca))\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Estimacion')\n",
    "    plt.legend()\n",
    "    plt.savefig(output)\n",
    "    #plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04 1.3812935072426558\n"
     ]
    }
   ],
   "source": [
    "seed = 11\n",
    "iteraciones = 200\n",
    "bandit = Bandit(seed = seed)\n",
    "agent = Agent(epsilon=0.1, seed=seed, k_arm=bandit.k)\n",
    "\n",
    "estimations = game(iteraciones, agent, bandit)\n",
    "estimations = estimations.reshape(iteraciones, bandit.k).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(estimations, output=\"sol7b.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](sol7b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ahora los valores medios dependen de la misma semilla y no por separado como anteriormente.\n",
    "* Las series de variables han cambiado por usar dos tipos de generadores y la intervencion del agente, ya que a la hora de elegir politica sobre los valores generados añade un nuevo factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
